# üéØ PLAN DE DIFERENCIACI√ìN TFM - MODELOS √öNICOS

## üö® PROBLEMA ACTUAL
- XGBoost = Ridge = **MISMAS 4 features** ‚Üí Resultados similares
- Todos usan **NSGA-II** ‚Üí Misma optimizaci√≥n ‚Üí Convergencia similar
- **TFM pierde valor cient√≠fico** si todos dan lo mismo

---

## üõ†Ô∏è ESTRATEGIA DE DIFERENCIACI√ìN

### **1. üìà MARKOWITZ (CL√ÅSICO) - Baseline**
```python
# ‚úÖ YA CORRECTO
- Input: Solo medias hist√≥ricas Œº = E[r] √∫ltimos 60 d√≠as
- Optimizaci√≥n: NSGA-II tradicional
- Filosof√≠a: Teor√≠a cl√°sica de portfolios (1952)
```

### **2. üå≥ XGBoost (ML TRADICIONAL) - Mejorado**
```python
# ‚ùå ACTUAL: 4 features b√°sicas
# ‚úÖ NUEVO: Features de ingenier√≠a financiera
features_xgb = [
    # Retornos multi-periodo
    "ret_1d", "ret_5d", "ret_10d", "ret_20d",
    
    # Volatilidad multi-escala  
    "vol_5d", "vol_10d", "vol_20d",
    
    # Momentum & Mean Reversion
    "momentum_5d", "momentum_20d", "rsi_14", 
    
    # Correlaciones cruzadas
    "corr_spy_20d", "corr_vix_20d",
    
    # Features t√©cnicos
    "ma_ratio_5_20", "bollinger_position",
    
    # Features macroecon√≥micos
    "vix_level", "vix_change_5d"
]
```

### **3. üìä RIDGE (ML LINEAL) - Diferenciado** 
```python
# ‚úÖ NUEVO: Features ortogonales a XGBoost
features_ridge = [
    # PCA de retornos sector-espec√≠fico
    "pca_tech_1", "pca_finance_1", "pca_health_1",
    
    # Ratios fundamentales sint√©ticos
    "price_to_ma_50", "price_to_ma_200",
    
    # Features de liquidez
    "volume_ratio_20d", "volume_spike",
    
    # Cross-asset features
    "beta_to_market", "correlation_to_bonds"
]
```

### **4. üß† LSTM/CNN (DEEP LEARNING) - Temporal**
```python
# ‚úÖ YA DIFERENCIADO: Ventanas temporales de 60 d√≠as
- Input: Secuencias [ret, momentum] √ó 60 d√≠as
- Captura: Patrones temporales complejos
- Arquitectura: Recurrente (LSTM) vs Convolucional (CNN)
```

---

## üéØ OPTIMIZADORES DIFERENCIADOS

### **Markowitz ‚Üí NSGA-II cl√°sico**
```python
objectives = [risk, -return]  # 2 objetivos tradicionales
```

### **XGBoost ‚Üí NSGA-III** 
```python
objectives = [risk, -return, turnover]  # 3 objetivos + diversidad
```

### **Ridge ‚Üí Optimizaci√≥n convexa**
```python
# Markowitz tradicional con r_hat de Ridge
# Soluci√≥n anal√≠tica m√°s r√°pida
```

### **LSTM/CNN ‚Üí Multi-objetivo robusto**
```python
objectives = [risk, -return, max_drawdown, skewness]  # 4 objetivos
```

---

## üîß IMPLEMENTACI√ìN INMEDIATA

### **FASE 1: XGBoost Mejorado (30 min)**
1. Crear `notebooks/xgb/02_preprocess_xgb_enhanced.ipynb`
2. A√±adir 16 features t√©cnicas + macro
3. Re-entrenar con LightGBM optimizado

### **FASE 2: Ridge Diferenciado (20 min)** 
1. Crear features PCA y fundamentales
2. Training independiente del dataset XGB

### **FASE 3: Validaci√≥n de Unicidad (15 min)**
1. Ejecutar `notebooks/validate_models.py` 
2. Verificar correlaciones < 0.95
3. Comparar distribuciones de r_hat

---

## üéØ M√âTRICAS DE √âXITO

### **Diferenciaci√≥n T√©cnica**
- Correlaci√≥n entre r_hat < 0.95 ‚úÖ
- Features diferentes por modelo ‚úÖ  
- Optimizadores √∫nicos ‚úÖ

### **Resultados de Backtesting**
- Sharpe Ratios diferenciados (¬±0.3)
- Max Drawdowns variables  
- Turnover patterns distintos

### **Progresi√≥n L√≥gica TFM**
```
Markowitz (œÉ=0.85) ‚Üí XGB (œÉ=0.95) ‚Üí Ridge (œÉ=1.05) ‚Üí LSTM (œÉ=1.15) ‚Üí CNN (œÉ=1.25)
```
*Progresi√≥n de complejidad = Progresi√≥n de performance*

---

## ‚ö° PR√ìXIMOS PASOS

1. **AHORA**: Implementar XGBoost mejorado
2. **Despu√©s**: Ridge diferenciado  
3. **Luego**: Validar unicidad
4. **Finally**: Backtest completo

**OBJETIVO**: Demostrar que **m√°s sofisticaci√≥n ML = mejor performance** 